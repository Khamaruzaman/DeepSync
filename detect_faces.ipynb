{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 604,
     "status": "ok",
     "timestamp": 1736089558776,
     "user": {
      "displayName": "KHAMARUZAMAN K M 21B771",
      "userId": "12093313641242443987"
     },
     "user_tz": -330
    },
    "id": "vkQti64NGzIB"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "module_path = '/content/drive/MyDrive/Colab Notebooks'\n",
    "sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3078,
     "status": "ok",
     "timestamp": 1736089565027,
     "user": {
      "displayName": "KHAMARUZAMAN K M 21B771",
      "userId": "12093313641242443987"
     },
     "user_tz": -330
    },
    "id": "hnSykzshHVUH",
    "outputId": "039d9a0d-8e21-4ec0-fd49-5777d751d843"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: facenet-pytorch in .\\.venv\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.0 in .\\.venv\\lib\\site-packages (from facenet-pytorch) (1.26.4)\n",
      "Requirement already satisfied: Pillow<10.3.0,>=10.2.0 in .\\.venv\\lib\\site-packages (from facenet-pytorch) (10.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in .\\.venv\\lib\\site-packages (from facenet-pytorch) (2.32.3)\n",
      "Requirement already satisfied: torch<2.3.0,>=2.2.0 in .\\.venv\\lib\\site-packages (from facenet-pytorch) (2.2.2+cu121)\n",
      "Requirement already satisfied: torchvision<0.18.0,>=0.17.0 in .\\.venv\\lib\\site-packages (from facenet-pytorch) (0.17.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in .\\.venv\\lib\\site-packages (from facenet-pytorch) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->facenet-pytorch) (2024.12.14)\n",
      "Requirement already satisfied: filelock in .\\.venv\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in .\\.venv\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (4.12.2)\n",
      "Requirement already satisfied: sympy in .\\.venv\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (1.13.1)\n",
      "Requirement already satisfied: networkx in .\\.venv\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in .\\.venv\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in .\\.venv\\lib\\site-packages (from torch<2.3.0,>=2.2.0->facenet-pytorch) (2024.12.0)\n",
      "Requirement already satisfied: colorama in .\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.0.0->facenet-pytorch) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in .\\.venv\\lib\\site-packages (from jinja2->torch<2.3.0,>=2.2.0->facenet-pytorch) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in .\\.venv\\lib\\site-packages (from sympy->torch<2.3.0,>=2.2.0->facenet-pytorch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install facenet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 9316,
     "status": "ok",
     "timestamp": 1736089579356,
     "user": {
      "displayName": "KHAMARUZAMAN K M 21B771",
      "userId": "12093313641242443987"
     },
     "user_tz": -330
    },
    "id": "-oBsQUnPFikU"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Type\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "import face_detector\n",
    "from face_detector import VideoDataset, VideoFaceDetector\n",
    "from utils import get_video_paths, get_method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 515,
     "status": "ok",
     "timestamp": 1736089584890,
     "user": {
      "displayName": "KHAMARUZAMAN K M 21B771",
      "userId": "12093313641242443987"
     },
     "user_tz": -330
    },
    "id": "-38t1t28FxsH"
   },
   "outputs": [],
   "source": [
    "def process_videos(videos, detector_cls: Type[VideoFaceDetector], selected_dataset, opt):\n",
    "    detector = face_detector.__dict__[detector_cls](device=\"cuda:0\")\n",
    "\n",
    "    dataset = VideoDataset(videos)\n",
    "\n",
    "    # loader = DataLoader(dataset, shuffle=False, num_workers=opt.processes,\n",
    "                        # batch_size=1, collate_fn=lambda x: x)\n",
    "                        \n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        batch_size=1,\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "\n",
    "    missed_videos = []\n",
    "    for item in tqdm(loader):\n",
    "        result = {}\n",
    "        video, indices, frames = item[0]\n",
    "        if selected_dataset == 1:\n",
    "            method = get_method(video, opt.data_path)\n",
    "            out_dir = os.path.join(opt.data_path, \"boxes\", method)\n",
    "        else:\n",
    "            out_dir = os.path.join(opt.data_path, \"boxes\")\n",
    "\n",
    "        id = os.path.splitext(os.path.basename(video))[0]\n",
    "\n",
    "        if os.path.exists(out_dir) and \"{}.json\".format(id) in os.listdir(out_dir):\n",
    "            continue\n",
    "        batches = [frames[i:i + detector._batch_size]\n",
    "                   for i in range(0, len(frames), detector._batch_size)]\n",
    "\n",
    "        for j, frames in enumerate(batches):\n",
    "            result.update({int(j * detector._batch_size) + i: b for i,\n",
    "                          b in zip(indices, detector._detect_faces(frames))})\n",
    "\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        print(len(result))\n",
    "        if len(result) > 0:\n",
    "            with open(os.path.join(out_dir, \"{}.json\".format(id)), \"w\") as f:\n",
    "                json.dump(result, f)\n",
    "        else:\n",
    "            missed_videos.append(id)\n",
    "\n",
    "    if len(missed_videos) > 0:\n",
    "        print(\"The detector did not find faces inside the following videos:\")\n",
    "        print(id)\n",
    "        print(\"We suggest to re-run the code decreasing the detector threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 618,
     "status": "ok",
     "timestamp": 1736089610590,
     "user": {
      "displayName": "KHAMARUZAMAN K M 21B771",
      "userId": "12093313641242443987"
     },
     "user_tz": -330
    },
    "id": "wb0k6UofgsUR"
   },
   "outputs": [],
   "source": [
    "# Define options manually instead of using argparse\n",
    "class Options:\n",
    "    dataset = \"DFDC\"\n",
    "    data_path = \"D:\\\\project\\\\archive\\\\Celeb-synthesis\"\n",
    "    detector_type = \"FacenetDetector\"\n",
    "    processes = 1\n",
    "\n",
    "opt = Options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 523,
     "status": "ok",
     "timestamp": 1736089599141,
     "user": {
      "displayName": "KHAMARUZAMAN K M 21B771",
      "userId": "12093313641242443987"
     },
     "user_tz": -330
    },
    "id": "Qum4cKM0Fz5Q"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument('--dataset', default=\"DFDC\", type=str,\n",
    "    #                     help='Dataset (DFDC / FACEFORENSICS)')\n",
    "    # parser.add_argument('--data_path', default='', type=str,\n",
    "    #                     help='Videos directory')\n",
    "    # parser.add_argument(\"--detector-type\", help=\"Type of the detector\", default=\"FacenetDetector\",\n",
    "    #                     choices=[\"FacenetDetector\"])\n",
    "    # parser.add_argument(\"--processes\", help=\"Number of processes\", default=1)\n",
    "    # opt = parser.parse_args()\n",
    "    opt = Options()\n",
    "    print(opt)\n",
    "\n",
    "\n",
    "    if opt.dataset.upper() == \"DFDC\":\n",
    "        dataset = 0\n",
    "    else:\n",
    "        dataset = 1\n",
    "\n",
    "    videos_paths = []\n",
    "    if dataset == 1:\n",
    "        videos_paths = get_video_paths(opt.data_path, dataset)\n",
    "    else:\n",
    "        os.makedirs(os.path.join(opt.data_path, \"boxes\"), exist_ok=True)\n",
    "        already_extracted = os.listdir(os.path.join(opt.data_path, \"boxes\"))\n",
    "        for folder in os.listdir(opt.data_path):\n",
    "            if \"boxes\" not in folder and \"zip\" not in folder:\n",
    "                if os.path.isdir(os.path.join(opt.data_path, folder)): # For training and test set\n",
    "                    for video_name in os.listdir(os.path.join(opt.data_path, folder)):\n",
    "                        if video_name.split(\".\")[0] + \".json\" in already_extracted:\n",
    "                            continue\n",
    "                        videos_paths.append(os.path.join(opt.data_path, folder, video_name))\n",
    "                else: # For validation set\n",
    "                    videos_paths.append(os.path.join(opt.data_path, folder))\n",
    "\n",
    "    process_videos(videos_paths, opt.detector_type, dataset, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ndmcKvjMF2RG",
    "outputId": "76d07edb-b38b-4da7-ed8c-a1a67cfd4618"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Options object at 0x000001B3865CA930>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5639 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'torchvision::nms' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'torchvision::nms' is only available for these backends: [CPU, Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\cpu\\nms_kernel.cpp:112 [kernel]\nMeta: registered at /dev/null:440 [kernel]\nQuantizedCPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\quantized\\cpu\\qnms_kernel.cpp:124 [kernel]\nBackendSelect: fallthrough registered at ..\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ..\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:324 [backend fallback]\nNamed: registered at ..\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ..\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ..\\aten\\src\\ATen\\native\\NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ..\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:53 [backend fallback]\nAutogradCPU: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:57 [backend fallback]\nAutogradCUDA: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:65 [backend fallback]\nAutogradXLA: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:69 [backend fallback]\nAutogradMPS: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:77 [backend fallback]\nAutogradXPU: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:61 [backend fallback]\nAutogradHPU: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:90 [backend fallback]\nAutogradLazy: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:73 [backend fallback]\nAutogradMeta: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:81 [backend fallback]\nTracer: registered at ..\\torch\\csrc\\autograd\\TraceTypeManual.cpp:297 [backend fallback]\nAutocastCPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\autocast\\nms_kernel.cpp:34 [kernel]\nAutocastCUDA: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\autocast\\nms_kernel.cpp:27 [kernel]\nFuncTorchBatched: registered at ..\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:720 [backend fallback]\nBatchedNestedTensor: registered at ..\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:746 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ..\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ..\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:158 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Execute the script\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[33], line 36\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# For validation set\u001b[39;00m\n\u001b[0;32m     34\u001b[0m                 videos_paths\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(opt\u001b[38;5;241m.\u001b[39mdata_path, folder))\n\u001b[1;32m---> 36\u001b[0m \u001b[43mprocess_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideos_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetector_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[31], line 36\u001b[0m, in \u001b[0;36mprocess_videos\u001b[1;34m(videos, detector_cls, selected_dataset, opt)\u001b[0m\n\u001b[0;32m     31\u001b[0m batches \u001b[38;5;241m=\u001b[39m [frames[i:i \u001b[38;5;241m+\u001b[39m detector\u001b[38;5;241m.\u001b[39m_batch_size]\n\u001b[0;32m     32\u001b[0m            \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(frames), detector\u001b[38;5;241m.\u001b[39m_batch_size)]\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, frames \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batches):\n\u001b[0;32m     35\u001b[0m     result\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;28mint\u001b[39m(j \u001b[38;5;241m*\u001b[39m detector\u001b[38;5;241m.\u001b[39m_batch_size) \u001b[38;5;241m+\u001b[39m i: b \u001b[38;5;28;01mfor\u001b[39;00m i,\n\u001b[1;32m---> 36\u001b[0m                   b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(indices, \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_detect_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m)})\n\u001b[0;32m     38\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(out_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(result))\n",
      "File \u001b[1;32md:\\project\\face_detector.py:41\u001b[0m, in \u001b[0;36mFacenetDetector._detect_faces\u001b[1;34m(self, frames)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_detect_faces\u001b[39m(\u001b[38;5;28mself\u001b[39m, frames) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List:\n\u001b[1;32m---> 41\u001b[0m     batch_boxes, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlandmarks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [b\u001b[38;5;241m.\u001b[39mtolist() \u001b[38;5;28;01mif\u001b[39;00m b \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch_boxes]\n",
      "File \u001b[1;32md:\\project\\.venv\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:313\u001b[0m, in \u001b[0;36mMTCNN.detect\u001b[1;34m(self, img, landmarks)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03mThis method is used by the forward method and is also useful for face detection tasks\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m>>> img_draw.save('annotated_faces.png')\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 313\u001b[0m     batch_boxes, batch_points \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_face\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_face_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthresholds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m boxes, probs, points \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m box, point \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_boxes, batch_points):\n",
      "File \u001b[1;32md:\\project\\.venv\\Lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py:79\u001b[0m, in \u001b[0;36mdetect_face\u001b[1;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[0;32m     76\u001b[0m boxes\u001b[38;5;241m.\u001b[39mappend(boxes_scale)\n\u001b[0;32m     77\u001b[0m image_inds\u001b[38;5;241m.\u001b[39mappend(image_inds_scale)\n\u001b[1;32m---> 79\u001b[0m pick \u001b[38;5;241m=\u001b[39m \u001b[43mbatched_nms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes_scale\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes_scale\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_inds_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m scale_picks\u001b[38;5;241m.\u001b[39mappend(pick \u001b[38;5;241m+\u001b[39m offset)\n\u001b[0;32m     81\u001b[0m offset \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m boxes_scale\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32md:\\project\\.venv\\Lib\\site-packages\\torchvision\\ops\\boxes.py:75\u001b[0m, in \u001b[0;36mbatched_nms\u001b[1;34m(boxes, scores, idxs, iou_threshold)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _batched_nms_vanilla(boxes, scores, idxs, iou_threshold)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_batched_nms_coordinate_trick\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\project\\.venv\\Lib\\site-packages\\torch\\jit\\_trace.py:1240\u001b[0m, in \u001b[0;36m_script_if_tracing.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1236\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m R:\n\u001b[0;32m   1238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tracing():\n\u001b[0;32m   1239\u001b[0m         \u001b[38;5;66;03m# Not tracing, don't do anything\u001b[39;00m\n\u001b[1;32m-> 1240\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1242\u001b[0m     compiled_fn: Callable[P, R] \u001b[38;5;241m=\u001b[39m script(wrapper\u001b[38;5;241m.\u001b[39m__original_fn)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\project\\.venv\\Lib\\site-packages\\torchvision\\ops\\boxes.py:94\u001b[0m, in \u001b[0;36m_batched_nms_coordinate_trick\u001b[1;34m(boxes, scores, idxs, iou_threshold)\u001b[0m\n\u001b[0;32m     92\u001b[0m offsets \u001b[38;5;241m=\u001b[39m idxs\u001b[38;5;241m.\u001b[39mto(boxes) \u001b[38;5;241m*\u001b[39m (max_coordinate \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(boxes))\n\u001b[0;32m     93\u001b[0m boxes_for_nms \u001b[38;5;241m=\u001b[39m boxes \u001b[38;5;241m+\u001b[39m offsets[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m---> 94\u001b[0m keep \u001b[38;5;241m=\u001b[39m \u001b[43mnms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes_for_nms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keep\n",
      "File \u001b[1;32md:\\project\\.venv\\Lib\\site-packages\\torchvision\\ops\\boxes.py:41\u001b[0m, in \u001b[0;36mnms\u001b[1;34m(boxes, scores, iou_threshold)\u001b[0m\n\u001b[0;32m     39\u001b[0m     _log_api_usage_once(nms)\n\u001b[0;32m     40\u001b[0m _assert_has_ops()\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\project\\.venv\\Lib\\site-packages\\torch\\_ops.py:755\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[0;32m    752\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[0;32m    753\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 755\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Could not run 'torchvision::nms' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'torchvision::nms' is only available for these backends: [CPU, Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\cpu\\nms_kernel.cpp:112 [kernel]\nMeta: registered at /dev/null:440 [kernel]\nQuantizedCPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\quantized\\cpu\\qnms_kernel.cpp:124 [kernel]\nBackendSelect: fallthrough registered at ..\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at ..\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:324 [backend fallback]\nNamed: registered at ..\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ..\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ..\\aten\\src\\ATen\\native\\NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ..\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:53 [backend fallback]\nAutogradCPU: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:57 [backend fallback]\nAutogradCUDA: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:65 [backend fallback]\nAutogradXLA: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:69 [backend fallback]\nAutogradMPS: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:77 [backend fallback]\nAutogradXPU: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:61 [backend fallback]\nAutogradHPU: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:90 [backend fallback]\nAutogradLazy: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:73 [backend fallback]\nAutogradMeta: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:81 [backend fallback]\nTracer: registered at ..\\torch\\csrc\\autograd\\TraceTypeManual.cpp:297 [backend fallback]\nAutocastCPU: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\autocast\\nms_kernel.cpp:34 [kernel]\nAutocastCUDA: registered at C:\\actions-runner\\_work\\vision\\vision\\pytorch\\vision\\torchvision\\csrc\\ops\\autocast\\nms_kernel.cpp:27 [kernel]\nFuncTorchBatched: registered at ..\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:720 [backend fallback]\nBatchedNestedTensor: registered at ..\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:746 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ..\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ..\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:158 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "# Execute the script\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPBRy27W1D3Kfy0Iz1goqSq",
   "gpuType": "T4",
   "mount_file_id": "1Wx_6mKFNc9dlwhrQMtzDKsVhgx5-XmjK",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
